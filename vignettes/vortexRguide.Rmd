---
title: "vortexR: user's guide"
author: "Carlo Pacioni and Florian Mayer"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vortexR: user's guide}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---




# Introduction
VortexR is an R package that automatises common post-Vortex simulation tasks, including collating Vortex output files, generating plots and conducting basic analysis (e.g. pairwise comparisons of scenarios) and more advanced statistics such as fitting of a Generalised Linear Model (GLM) to investigate the main and the interaction effects of the variables of interest. The user enters minimal information (e.g. directory, name of the project and Vortex running mode) using arguments in R functions. In some instances, VortexR will behave differently depending on the data that are fed in. Function-specific details are provided in the following sections of the manual.

# Installation
Need to be completed.
If you are at your first experiences with R, you may find these few steps helpful. 
Go on www.r-project.org and download the latest version compatible with your operative system.
While there is no strict requirement to do so, we find that using R with RStudio (www.rstudio.com) is very handy, and it is also free! It will be very easy to keep clean and then capture your sequence of commands, including possible data manipulations, so that you can return to it later. If you want to give it a try download also the latest version of RStudio. 
From within R (or RStudio), install and load VortexR using the command: TODO and you are good to go!
R stores (in your computer's physical memory) the data in objects (vectors, data frames, lists, etc.) and you will be hearing these names a lot throughout this manual. If you are unsure what these are, or need any help to get around R and RStudio there are plenty of materials on line. A point where to start can be reading paragraph 1, 2, and 6 of A very (short) intro to R. 
Many objects generated by VortexR are data.table (M Dowle et al. 2014). Data.tables are enhanced data frames and make several operations much faster. The user does not need to use this package (if the package in not loaded, R will handle these objects effectively as data frames), but after the initial learning curve, there may be some benefits when manipulating large datasets. A few examples are provided below and the users can find additional information on www.cran.r-project.org/web/packages/data.table/index.html. 
VortexR has been developed on R v 3.1.2 and, while it may work on previous R versions, this has not been tested and the user is strongly recommended to use v3.1.2 or greater. Similarly, VortexR was developed to be compatible with Vortex v10 (Lacy & Pollak 2013). Formatting of the outputs is slightly different in Vortex v10 compared to previous versions, which may cause issues. Some functions may process adequately outputs from previous versions, but it is responsibility of the user to make sure the correctness of the results. 
VortexR makes heavy use of several R packages. Below there is the comprehensive list. Typically, these packages are installed manually from R, but the user can use the VortexR utility Check_Packages.R , which will check if relevant packages are installed and will install the ones that are missing.
R packages used by VortexR
Reference
ggplot2 
NEED TO UPDATE THIS LIST!!!
grid 

data.table 

irr 

plyr 

gridExtra 

quantmod 

glmulti foreach 

doSNOW 

R.utils 

betareg

 
If you are using Windows, you may have problem installing and loading glmulti package and/or rJava. This is most likely because you do not have Java installed or are not using the same version as R. In other words, if you are using a 64 bit version of R, you also have to have installed a 64 bit version of Java. Note that if you have Java installed and use the Java website to check whether you have an up to date version of Java, it will report it as 'up-to-date' if you have the latest version of Java, but will not check whether it is a 32 or 64 bit. If you have a 32 bit version and use the 64 bit version of R, R will not manage to install rJava, which is a required package to use glmulti. The easiest may be to navigate to your Program Files folder (or Program Files (x86) if you are using a 32 bit version of R) and check whether you have a current version of java in the "Java" folder. 
Remember to put where the sample data is located and what it is. 

Manual style and structure
Different fonts and styles were used throughout the manual :
Name of VortexR functions (e.g. CollateDat)
"Names of the files" (e.g. "MyFile.txt")
Commands for R prompt (e.g. getwd() ) or function arguments (e.g. project)
The symbol # proceed comments that are not part of the commands, but that will be ignored by R if the user copy and paste these in the R prompt  
A worked examples (before the description of an example)
--> to indicate items within a drop down menu (e.g. File --> Change dir...) 

We generally refer to the extension of Vortex output file to identify the type of the file (e.g. .yr; .dat; .stdat etc.). We may refer to the .yr files as "census" files, following Vortex documentations. If unfamiliar, the reader should refer to Vortex manual to clarify the data that are reported in each file type.
The main section of this manual (VortexR components and workflow) is divided in three parts. The first (A quick run down) simply provides a quick overview of the possible workflow in VortexR. The second (Main functions) describes what we define as main functions. These are functions that execute primary operations such reading output files or conducting statistical analysis. The third section (Utilities) describes minor functions that either calculate specific parameters (e.g. effective population size) or carry out data handling such as converting the data format to make the data fit VortexR functions' requirements or collate together data files generated from VortexR itself (e.g. merging data generated with a ST and non-ST runs). Most function descriptions are followed by a worked example. 
In the appendix, there is a brief summary of the functions in a style that will be familiar to R users. This section is meant to provide a quick overview of the functions, mainly in relations to the available arguments and syntax. The user is strongly encouraged to read the main section of the manual as it contains a more detailed description of functions' operations and caveats. 
Licence and Citation
VortexR and its manual are distributed free of charge under the XXX licence. We only ask you to please cite the following paper if you use VortexR for your work. If you customise a VorteR script to suit your specific needs, we would appreciate if you can still cite VortexR's main reference stating something like: "we used a customised version of the script VortexR_Script_name (Pacioni and Mayer, in prep)".

Pacioni C, and Mayer F. in prep. VortexR: a R package for post Vortex simulation analysis.
Please note that no commercial use of any part of VortexR is allowed.

Disclaimer
While all reasonable efforts have been made to ensure accuracy of VortexR calculations and the text of this manual, no responsibility is taken for unintentional errors or problems that may be encountered by the users.

Technical support and feedback
Please report possible bugs, unexpected behaviour, suggestions and positive or negative feedback to ... .
Feel free to email  us directly if you need assistance, though we cannot guarantee that we can provide individual assistance. Do you agree? Mailing list??

# VortexR components and workflow - A quick run down
Vortex v10 is an incredibly versatile tool that is used by ecologists and geneticists to explore a large number of ecological and genetic questions. It is very difficult to predict what analytical approach Vortex users may want to undertake as it will be highly influenced by how the simulations were set up and the questions been tackled. With the release of Vortex v10, the users can easily develop a large number of scenarios to explore the effects of the factors being studied as well as their interaction. This is particularly true when the user takes advantage of the new Sensitivity Test module (ST), which can automatise the creations and executions of hundreds of scenarios. Therefore, an equivalent automatization of the post-simulation analytical steps is also required as it may become quickly unfeasible to manually access and analyse such a large number of files and data. We developed a set of functions that execute operations on various Vortex outputs. We believed that these tasks will be of common utility for Vortex users' community; however it is the user's responsibility to evaluate their suitability. A possible workflow is described in Figure 1. Once the user has run the simulations in Vortex, there will be a collection of output files in a "VOutput" folder. A first step may be to collate the data files that are of interest into one database, so that data can be easily accessed by R or other statistical or graphic software. Different data types will require different VortexR functions. For example, with the function CollateDat the user can collate together all the .dat or .stdat files from a specific project (and scenario in case of the .stdat files). Additional Vortex output files that can be collated together are the .yr and the .run files from a ST run (using CollateYr and CollateRun respectively). Secondly, the user may want to generate a number of plots to visualise general trends in the simulations and inspect whether the models behaved as expected (List of functions here). If everything looks OK the user may move on to the analysis of the data. Pairwise will conduct pairwise comparisons (details on the statistical aspects are provided below) between all the scenarios imported into VortexR against a baseline scenario that will be automatically identified for a ST run (using the suffix appended by Vortex, Base) or is specified by the user for a normal (non-ST) collection of scenarios. Complementarily, the user may want to fit a regression model to evaluate interaction effects in addition to the main effects of the variable being investigated (Regression). Once the user has an idea of the important aspects of the analysis, he or she may select some of the previously generated plots and modify them to adapt them to the formatting style required for publication or use in other documents.

Collaterally, there are a number of functions to conduct minor calculations (e.g. calculation of effective population size from demographic or genetic data) or further data handling that are described in more details in the Utilities section. 

If you do not want to check out each of the functions described below but you want to quickly have a pragmatic approach and get VortexR to do something to see if you like it or not, then do the following:
# Install and load the package typing in the R prompt: 
install.packages()
require(VortexR)
# Type in the R prompt: 
example(VortexR) - This will be the help command to run examples, you will be asked to set a directory where to store the results

Now read below to know what you have done...
VortexR comes with some data generated with a few Vortex projects that we will be using as examples. A brief description of these projects is reported in the section below, more details are in REFS. 
Once you run example(VortexR), you will see that a number of folders have been created. VortexR saves data to disk in two formats, as a text file and as a R object (.RData), and saves plot in .RData and PDF format. We are going to check only a couple of these files as an example, and we will be using either the txt or the PDF files. All this may not be particularly meaningful for you right now, but with your own models, you will be familiar with the modelling settings and different scenarios and (hopefully) it should be easier to read and interpret these results.
ProcessedData
The folder 'ProcessedData' contains the collated Vortex outputs. That is, for each scenario, all the files of same type (e.g. .dat files) created by Vortex are merged together in one database and are saved to disk in two formats, as a text file and as a R object (.RData). In your preferred spreadsheet software, open the file 'Pacioni_et_al_ST_Classic_data.txt'. Following Vortex style, this is a semicolon-delimited text file. You will quickly realise that it is a congregated database of mean parameter values for each scenario run in the ST run 'ST_Classic' (in this case, 23 scenarios plus the baseline ). 
Now open 'Pacioni_et_al_ST_Classic_CensusMeans.txt'. In case you have movements (e.g. migration or translocations) in your simulations, you may be interested in obtaining the mean of animals that moved between the populations, or you may be interested in the mean number of individuals by gender and age. This file contains these calculated mean values.
Lastly, open 'Pacioni_et_al_ST_Classic_LookUpT.txt', this is a simple table that summarises the parameter values for each scenario run within a ST run (e.g. SV1, SV2...) and user-made population variables (e.g. PS1, PS2...).
All the other files are in similar to those you just opened and are mainly used as input for other function. 
Plots
The folder 'Plots' contains a number of plots generated with the default values. A number of changes can be done to customise these. 
Open 'Pacioni_et_al_ST_Classic_Dotplots.pdf', this is a dot plot of mean values with standard deviations. Parameter values are on the Y-axis and scenarios are on the X-axis. Each parameter is in one page. Each page is divided in columns, where each column reports the mean value for a specific year (in this case we set 80 and 120). In case you have more than one population, the page is divided in rows, one for each population.
The plots 'Pacioni_et_al_ST_Classic_YearVsParamsPlots.pdf' and 'Pacioni_et_al_ST_Classic_YearMidVsParamsPlots.pdf' are line plots the represent the changes of the parameter through simulated years. The only difference is that the second plot is limited from the beginning of the simulation and a 'Mid' year, in this case 50. Clearly these plots are not very legible with so many scenarios, but they will give you an idea of what's going on and you can subset the scenarios to reduce their number before running the relative plot function.

DataAnalysis
The folder 'DataAnalysis' contains a number of files that report results of pairwise comparisons. Open the file 'Pacioni_et_al_ST_Classic.SSMD.table.pvalues.txt' to see what scenarios are statistically different from the baseline scenario using the strictly standardised mean difference (see details on Pairwise function to know more about this). In this example, we used the population size (Nall) and heterozygosity (Het) at year 120 to compare the scenarios. Similar comparisons are conducted using also sensitivity coefficients. A slightly different approach is reported in 'Pacioni_et_al_ST_Classic.mean.SSMD.table.pvalues.txt '. Here the mean effect of the seven parameters that modified during the ST run (from SV1 to SV7) is evaluated. 
If you open 'Pacioni_et_al_ST_Classic.ranks.mSSMD.txt' you will find two columns that report the rank for each scenario based on the mean strictly standardised mean difference for each of the two parameters considered.  
Further analysis include a Kendall's coefficient of concordance (see verify whether the ranking is in agreement between different parameters) and calculations of mean effective population size to census population size ratio (see 'NeNRatio.csv') from year 50 to 120. These ratios are manually calculated with the output of the functions Ne and Nb.

Regression
To evaluate the main and interaction effects of some of the parameters used to set the simulations, we used the function Regression fit a regression model to the data. The results of this analysis are in the folder 'Regression'. Wethere are the results of 




A brief description of the example data
Pacioni et al developed a baseline PVA model for the woylie (Bettongia penicillata). Then they conducted a sensitivity analysis by varying the carrying capacity, the mortality rates for each age class (by multiplying them by a factor from 2 to 6), the standard deviation of these rates (expressed as a proportion of the mean, range 0.1-03), the mate monopolization and the initial population size. Then the looked at the individual impact of each of these parameters (in what we called a classic approach where each parameter is modified, one at the time) as well as at their interaction. For the latter, they needed to vary these parameters concurrently, and they used a ST scenario in Vortex with a Latin Hypercube Sampling. Note that the data for Pacioni et al are just a subset of the original (only three runs for 120 years, and only for a subset of scenarios), so your results may not match the ones reported in the original publication. 
Campbell et al ... evaluated starling (Sturnus vulgaris) demographic projections under different management scenarios....


Use of VortexR
All VortexR functions assume that the user has set the working directory (setwd() ) where relevant Vortex output files are located. Alternatively, the path to the raw data can be passed with the argument filepath.
A worked example:
# The following command will set the working directory to VOutput (where Vortex output files are)
setwd("C:/Users/30373314/Documents/TranslocationPVA/VOutput")
# To check what the working directory is, you can use
getwd()
[1] "C:/Users/30373314/Documents/TranslocationPVA/VOutput"

Vortex will return the results of the functions to R and can be used for subsequent VortexR functions or further data analysis and manipulations in R . VortexR also write the results to disk in two formats, a .RData file (which offer the advantage of retaining all R information about the data and can be read back in R if you need to) and text files, that following Vortex philosophy, have semicolumns (";") as separators. The text files can be imported in other statistical packages or software as required. Plot function use PDF format as secondary file format rather than text file. The user can avoid VortexR to write the results to disk passing FALSE to the argument save2disk, otherwise, depending on the function, VortexR will store the results in one of the following folders: ProcessedData, DataAnalysis, Plots, Regression.

Main functions
Collate
CollateOneDat & CollateDat
These functions collate one or all the .dat or .stdat files within a folder. CollateOneDat  takes only two arguments: the name of the file (filename) and the number of runs or iterations (runs) that were conducted, while CollateDat has four arguments: the name of the project (project), the name of the scenario (scenario), whether it is a ST run (ST) and the number of runs or iterations (runs) that were conducted. There is no need to pass the number of populations or their names as VortexR will automatically detect this information from the files. 
scenario is used to indicate the name of the scenario that you want to import if you select ST=TRUE (you may have more than one ST scenario within the same project). If ST=FALSE then scenario is ignored.
There are some assumptions that the user has to respect when using this function. If ST is set to FALSE, it assumes that all .dat for the indicated project need to be read in (NOTE: VortexR will not look into subfolders, so separating files in subfolders may be a way to select files to be used in different analysis).
CollateDat assumes that all scenarios have the same number of columns (variables) being recorded by Vortex (this is something that may change in future version of VortexR). This is important if you have used your own IS, PS or GS because these are output and may alter the number of columns between scenarios. If this is your case, for now, you can either select only files that have the same headings (by moving the others in a subfolder) or you can create dummy variables before running Vortex in order to match the headings between scenarios (if feasible). Somewhat related to this issue, if you receive an error message saying "Error in match.names", then the problem is that the headings of the column(s) of one file (most likely the successive to the last file read, which should be displayed in the console) do not match with previous files. This may happen when you did not use the same name(s) for variable(s) that you created (e.g. PS or IS). Another possibility is that you may still have some "test" (old) files that was left behind from when you were testing/developing your model, but still with the same project (and scenario) name. Make sure all the files that are supposed to be read are current files (i.e. all produced from the same version of the project) and with the same headings.
It is important that all the scenarios being imported in VortexR with this function have the same number of iterations. It is probably a good practise to check the .sum file to make sure you didn't forget to set the same number of runs once you have developed your final model. The number of iterations that you indicate with the argument runs, in fact, will be used to calculate the standard deviation for the probability of extinctions and survival (extant), which are not provided by Vortex in the .dat or .stadat files, but that you may need if you are interested in testing the difference of these parameters between scenarios using Pairwise.
CollateDat will create a folder named "ProcessedData" (if not existing yet) where the collated data is saved in, and will output two files: a .RData file, and a text file. Both files's names will have the suffix "_data" and the contain the same data: a large database that replicates the data found in the .dat or .stdat with the difference that these are not separated in blocks, one for each population, but are unified with an extra column with the population name and a second column with the name of the scenario.
A worked example:
# Read data from 'Pacioni_et_al_ST_Classic(Base).stdat' and store the output
# in the object 'col.classic.base', which is available in memory. 
# The function will also save two output files (see above).
col.st.classic <- CollateOneDat("Pacioni_et_al_ST_Classic(Base).stdat", 3)

# Read data from all .stdat of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_Classic' and store the output in the object 
# 'col.st.classic'
col.st.classic <- CollateDat(project="'Pacioni_et_al ",   					scenario="ST_Classic", ST=TRUE, runs=3, filepath=getwd())


CollateRun
This function collates together output files that are generated during a ST with the extension .run. The main purpose of CollateRun is to prepare these data for Regression (see below) when the user is interested in conducting analysis on genetic parameters (which are not reported in the census files (.yr). Any of the parameters summarised in .run can actually be passed to Regression, and possibly the user may want to use CollateRun when he or she is interested in the census size (N) at the end of the simulation (.run files only report parameter at the end of the simulation, so the user cannot use them to analyse other 'years' of the simulations). The .run files are smaller because they do not report data for each year as the .yr do and therefore CollateRun is much faster than CollateYr. The user has to only pass the argument numPops and the usual project and scenario. The argument numPops takes an integer that indicates the total number of modelled populations including the metapopulation. The function automatically selects all the .run files within the selected folder for the indicated project and scenario and collates the data together. It outputs a list with two elements: the first (run) is a dataframe with a similar layout to .run file with the addition of a column with the scenario name. The second element (lrun) reports the same data in a 'long' format. Here, the data from different populations are stacked under the common variables and a column is added with the name of the population to which the data are related. The second element is the input for Regression. The function writes each element of the list to disk in two formats, .RData and .txt, in the folder "ProcessData". The suffix "_run" identifies the first element and the suffix "_lrun" the second (where the data are rearranged in long format).

A worked example:
# Run CollateRun on all .run of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_LHS' in the selected folder.
# store the result in 'run'
run <- CollateRun(project="Pacioni_et_al", scenario="ST_LHS", numPops=1)

# Remove base scenario from the putput in long format
lrun.ST_LHS.no.base <- run[[2]][!run[[2]]$Scenario == "ST_LHS(Base)",]

CollateYr
CollateYr collates together all the census files (.yr) for the indicated project and ST scenario (or only one file if R is pointed to a folder that contains only one file). It also calculates the mean, across iterations, of each parameter for each scenario reported in these files.  It takes three arguments: numPopsNoMeta and the usual project and scenario. The first is the number of the populations contained in your project. Be careful here, because Vortex does not include the metapopulation in the .yr files, so you do not have to count that in the total number of populations in your project when using this function. Census files may be relatively large if you have long simulations and used several state variables in your Vortex project, so running this function may take up lots of memory. This should improve in the near future because VortexR will, almost exclusively, make use of the package data.table to read files, but currently there are some issues in implementing a few functions offered by this package (see issue #801 on https://github.com/Rdatatable/data.table/issues). A possible workaround at the moment is to divide the .yr files in subfolders, and run CollateYr one folder at the time. You can then use CollateProcData to unify the dataframe.
The output of this function is a list with two elements. The first (Census) is a dataframe that looks like a standard census file except that has, for each row, the name of the scenario and the iteration (i.e. two extra columns). Also, the columns names (headings) are modified to include a suffix with the number of the population. These data can then be used in Regression after having rearranged the formatting with the function ConvLYr (see below). 
The second element (CensusMeans) has the same layout but reports the mean of each parameter across all iterations, for each simulated year. CensusMeans (which is in reality a data.table object) can be used to monitor the mean number of supplements or migrants etc. for each simulated scenario. 
The function writes each element of the list to disk, in the folder "ProcessData", in two formats, .RData and .txt. The suffix "_Census" identifies the first element and the suffix "_CensusMeans" the second. 
A worked example:
# Run CollateYr on all .yr of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_Classic' in the selected folder and 
# store the result in ' yr.st.classic' 
yr.st.classic <- CollateYr(project="Pacioni_et_al", scenario="ST_Classic",
                           numPopsNoMeta=1)
ConvLYr
This function rearranges the data contained in the first element (Census) of the list generated by CollateYr in long format (Convert to Long Year). The results are saved, in the folder "ProcessData",  in the two usual formats (.RData and .txt) with suffix "_lcensus". The data are passed with the argument data and the familiar project and scenario are used in this function too. The user has to also indicate the number of populations in the project without the metapopulation (with the argument numPopsNoMeta) and the years that would like to retain in the analysis with the argument yrs – if more then one year is required, these may be requested defining a numerical vector, e.g. yrs=c(10, 20, 30). This function has been developed to feed the data to Regression and because the tasks carried out are memory hungry, the function will thin the data to leave only the years that the users is planning to use in the analysis carried out by Regression. However, the user can retain all simulated years in the output if needed for other purposes. For example, assuming that 100 years were simulated, using yrs=1:100 would retain all of them in the output. With the argument appendMeta, the user can ask (by using TRUE or FALSE) to perform the necessary calculations to include the metapopulation (i.e. sums the census data across all the populations). If you are having memory issues when running this function, a possible workaround at the moment is to divide the .yr files in subfolders, run CollateYr and ConvLYr for one folder at the time. You can then use CollateProcData to unify the dataframe. During the testing of this function, we only had problem when working with more than 300 files with 10,000 iterations each, simulating 50 years (on a machine equipped with i5 processors, 8 GB of RAM and using Windows 7). So, unless you are close to these numbers, you should be fine to process all the .yr at once. 
A worked example:
# Assuming that your are following on from the previous function where 
# the output of CollateYr was stored in yr...
lyr<-ConvLYr(data=yr.st.classic[[1]], numPopsNoMeta=1, appendMeta=F,
	project="Pacioni_et_al", scenario="ST_Classic", yrs=120)

Plots
Vortex v10 offers a fantastic new graphic interface, and the user may not need to generate additional plots. The main advantage of the plot functions implemented in VortexR is that it can automatise the production of plots if the user wants, for example, the same plot for different years in the simulation, or only for a subset of populations avoiding the manual selection of options that may be required in Vortex. It should be noted that as from V10, Vortex offers a very large number of plot types and VortexR is not intended to be a replacement. While the main purpose of the plots generated by this function is exploratory, each plot is saved as an R object (with the exception of MScatter) and can be sub sequentially manipulated by the user. In fact, all the plots saved by VortexR in the .RData files (see below) are ggplot outputs and these can be further modified by the user (after having loaded the ggplot package with library(ggplot2) ). 

VortexR tries to resize the text of the labels in the legend and on the x-axis to make them fit, but there will be situations where this is just impossible (e.g. where there are tens of scenarios to be plotted).

All plot function use the output from CollateDat as input, which is passed with the argument data. The function MScatter can also take the output of ConvLYr or CollateRun (in long format).

LinePlotYear
LinePlotYear takes the usual arguments project, scenario and ST, which are provided only to name the output files so that the user can recognise their content. With the argument params, the user has control over the parameters that are plotted, which are passed as character vector, while plotpops is used to select the population(s) to be plotted. The user can pass a character vector (e.g. plotpops=c("pop1", "pop2") ) of any length to specify the populations to be included in the plot, or can use plotpops="all" (default) to include all the simulated populations (including the "metapopulation" if present) and avoid to type each name. 
The output of LinePlotYear will be saved in a folder called "Plots". 
VortexR will save a PDF named "Project_(Scenario)_YearsVsParamsPlots.pdf" that contains a plot for each population and each parameter that the user listed against all simulated years. Additionally a .RData file named "Project_(Scenario)_YearsVsParamsPlots.RData", which contains an R object for each parameter that is named following the same pattern described above (i.e. collating the name of the project, the scenario (if ST=TRUE) and the parameter). After loading in R the .RData file, the list of the objects contained in memory can be obtained with ls().The same (ggplot) objects will be stored as a list in the object where the function was pointed.

A worked example:
# Read data from all .stdat of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_Classic' and 
# store the output in the object 'col.st.classic', 
# which is needed to run the plot functions
col.st.classic <- CollateDat(project="Pacioni_et_al", scenario="ST_Classic",
                             ST=TRUE, runs=3, filepath=getwd())

# Plot data
lineplot.st.classic <- LinePlotYear(data=col.st.classic, 							project="Pacioni_et_al", scenario="ST_Classic", 
				ST=TRUE, 
				params=c("PExtinct", "Nextant", "Het", "Nalleles"))

LinePlotYearMid  
LinePlotYearMid  is a function that performs a very similar task to LinePlotYear but uses the argument yrmid to set the last year to plot (i.e. this function plots the parameters from year 0 to yrmid). The purpose of this plot is to "zoom" in the initial phase of the simulations to better appreciate dynamics of the parameters of interest. The outputs will be saved with the name "Project_(Scenario)_YearMidVsParamsPlots.pdf"  and "Project_(Scenario)_YearMidVsParamsPlots.RData". 

A worked example:
# Read data from all .stdat of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_Classic' and 
# store the output in the object 'col.st.classic', 
# which is needed to run the plot functions
col.st.classic <- CollateDat(project="Pacioni_et_al", scenario="ST_Classic",
                             ST=TRUE, runs=3, filepath=getwd())

lineMidPlot.st.classic <- LinePlotYearMid(data=col.st.classic, 
					project="Pacioni_et_al", 
					scenario="ST_Classic", ST=TRUE, yrmid=50,
					params=c("PExtinct", "Nextant", "Het", 						"Nalleles"))


DotPlot
A third plot function is called DotPlot.  The user has the option of selecting one or more time horizon with yrs(e.g. yrs=c(50,100) )  as long as these values are passed as integer and within the range 0 to max, where max is the last year of the simulation. VortexR will generate a plot for each point in time passed with yrs. The outputs of this function contain dot plots of mean parameter values for each population (row) at each year value requested (columns) with bars representing the standard deviation. You can control which variable is used to set the colours of the plot with the argument setcolour. The default option is to use the scenarios. Remember that if you pass a continuous variable, R will assign a continuous gradient of colour to the marker (e.g. for example, a scale from blue to black). If you want to obtained a sharp change of colours between different values of your variable, you can use (or format your variable as) a factor (see example below). It also possible to change the shape of the markers based on a variable value, although this can only be done for variables that are factors (or integers with a limited number of values) and may require the user to manipulate the input data beforehand. Similarly to previous functions, the function saves of files "Project_(Scenario)_Dotplots.pdf" and "Project_(Scenario)_Dotplots.RData".


A worked example:
# Read data from all .stdat of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_Classic' and 
# store the output in the object 'col.st.classic', 
# which is needed to run the plot functions
col.st.classic <- CollateDat(project="Pacioni_et_al", scenario="ST_Classic",
                             ST=TRUE, runs=3, filepath=getwd())

dot <- DotPlot(data=col.st.classic, project="Pacioni_et_al", 					scenario="ST_Classic", ST=TRUE, yrs=c(80, 120),
			params=c("PExtinct", "Nextant", "Het", "Nalleles"),
			setcolour="scen.name")


If you want to change the shape of the marker to be triangles, you can type in your R prompt:
# Assuming that you called your object that contains 
# one of the plot (say Heterozygosity) plotHet
plotHet <- dot[[3]]
plotHet+geom_point(shape=2) 

If you want to change the shape based on a factor contained in your data, you may have to manipulate the data. In Pacioni et al in prep this is a different Pacioni et al from the previous one (which should become clearer once I compile the final ref list). I didn't manage to get the data ready for this yet. I'm not sure whether we should keep this in for even if it is not possible to run the examples, or we should remove it and put that back in once the paper is submitted and the data are ready (which i think should be only in .rda), SV1 and SV2 are variables included in a ST scenario.  SV1 represents the number of individuals released to establish a population and SV2 is the time intervals at which these releases occur. We want to change SV1 into a factor so that each value is plotted in different colours (if we leave it as continuous variable is plotted as a gradient between two colours). Secondly, we want to change the shape of the markers based on the time intervals between releases, but we want to change these into 'customised' values in order to express these as number of releases over two generations. Lastly, we want to add a legend that explains all this. Assuming that your data is in the object called 'dat':
SV1f<-as.factor(dat$SV1) # Make SV1 a factor


# Create a new character variable to change values of SV2
Frequency<-as.character(dat$SV2) 
Frequency[dat$SV2==2]<-"10"
Frequency[dat$SV2==4]<-"5"
Frequency[dat$SV2==8]<-"2"
Frequency[dat$SV2==32]<-"1"
Frequency[dat$SV2==1]<-"0"
Frequency<-as.factor(Frequency)

# Then convert this into a factor and order the levels
Frequency<-factor(Frequency, levels=c("10", "5", "2", "1", "0"))

# Insert the new variables into the existing dataframe
dat<-cbind(dat, SV1f, Frequency)

# Usual DotPlot call but note that now setcolour is set to SV1f
plots<-DotPlot(
  data=dat,
  yrs=80,
  params=c("Het", "Nalleles", "Nall"),
  project="TranslProg",
  ST=TRUE, 
  scenario="ST_EstablishOneSource",
  setcolour="SV1f",
  plotpops=c("Recipient")
)

# Store individuals plot in separate objects
plotHet<-plots[[1]]
plotNa<-plots[[2]]
plotNall<-plots[[3]]



The data are subsetted by DotPlot to select only yrs and plotpops. If you want to pass a factor to change the shape it has to be of same length of the data.frame subset, hence the two following calls make the factor "Frequency" of same length of the data in the dataframe used by DotPlot.

datplot <- subset(dat, pop.name %in% "Recipient")
datplot <- subset(datplot, Year %in% 80)

# now you can use 'Frequency' to adjust the shape of the markers
# name= is used to insert the legend title
plotHet<-plotHet+aes(shape=Frequency)+scale_shape(name="Number of releases/nover two generations")+
  scale_colour_discrete(name="Number of/nindividuals")+
  theme(strip.text.x=element_blank(), strip.text.y=element_blank(), strip.background=element_blank(),
        axis.text.x=element_blank() ) +
          ylab("Heterozygosity") 

plotNa<-plotNa+aes(shape=Frequency)+scale_shape(name="Number of releases/nover two generations")+
  scale_colour_discrete(name="Number of/nindividuals")+
  theme(strip.text.x=element_blank(), strip.text.y=element_blank(), strip.background=element_blank(),
        axis.text.x=element_blank() ) +
  ylab("Number of alleles") 

plotNall<-plotNall+aes(shape=Frequency)+scale_shape(name="Number of releases/nover two generations")+
  scale_colour_discrete(name="Number of/nindividuals")+
  theme(strip.text.x=element_blank(), strip.text.y=element_blank(), strip.background=element_blank(),
        axis.text.x=element_blank() ) +
  ylab("Population size") 

# Lastly, you can save a pdf with the plots you have created
pdf(file = "Dotplots.pdf")
print(plotHet)
print(plotNa)
print(plotNall)
dev.off()

You should have something like the figure below.


MScatter
MScatter generates a matrix of scatter plots. This is useful when the possible associations between variables want to be graphically inspected. In addition to the output from CollateDat, the function MScatter can also take the output of ConvLYr or CollateRun (in long format) as input, which is passed with the argument data. The type of input data is indicated with the argument data.type. Possible options for this argument are "dat", "yr" or "run". The default is data.type="dat". It may take a while for R to generate an output from this function, depending on the number of parameters. When using "yr", or "run", it will take even longer because typically these input data have a larger number of data points. 
The variables to be plotted are passed with the arguments param and vs. The variable passed with param will be plotted last, so if the user is plotting variables included in a regression model, it may be convenient to pass the dependent variable with param so that the pairwise scatter plots with this variable will be all in one line. It may happen that the input data does not contain the all set of variables that are to be plotted (this often happens with "run" because this output only reports four variables). In these situations, the user can either manipulate the data beforehand and pass the complete data set where the additional variables have been added as new columns, or can pass a 'look up table' with the argument lookUp. The variables contained in the look up table will be matched using the scenarios names (alone, so variables that change values at each year or iteration cannot be passed with this argument but need to be manually inserted in the dataframe). Another situation when the users may want to pass the variables with the argument lookUp is when some of the populations go extinct. This is because in other outputs (e.g. .yr) these parameters take value 'zero' when the populations are extinct. By using the output from LookUpTable, the parameters are evaluated at year 0 and should then take the appropriate values. 
The user also has to indicate the year to plot (with the argument yr) and the population(s), which are indicated using the number of the population (e.g. 1; with the argument popn).

A worked example:
# Read data from all .stdat of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_LHS' and 
# store the output in the object 'col.ST.LHS', 
# which is needed to run the plot functions
col.ST.LHS <- CollateDat(project="Pacioni_et_al", scenario="ST_LHS",
                           ST=TRUE, runs=3, filepath=getwd())

# Remove base scenario
col.ST_LHS.no.base <- col.ST.LHS[!col.ST.LHS$scen.name == "ST_LHS(Base)",]

# Use function LookUpTable to obtain correct parameter values at year 0
lkup.ST_LHS <- LookUpTable(data=col.ST_LHS.no.base, project="Pacioni_et_al",
                        scenario="ST_LHS", ST=TRUE,
                        pop="Population 1",
                        SVs=c("SV1", "SV2", "SV3", "SV4", "SV5", "SV6", 				"SV7"))

scatter.plot <- MScatter(data=col.ST_LHS.no.base[1:33], data.type="dat",
				lookUp=lkup.ST_LHS, yr=120, popn=1, param="Nall",
				vs=c("SV1", "SV2", "SV3", "SV4", "SV5", "SV6", 
				"SV7"))

Data analysis
Pairwise
Pairwise runs a relatively simple analysis of the data that is based on pairwise comparisons against a baseline scenario using sensitivity coefficients (SC, Drechsler et al. 1998) and strictly standardised mean difference (SSDM, Zhang 2007), described in more details below. Several arguments can be used in this function. Other than the already-met project, scenario and ST, the user has to specify the type of ST (if ST=TRUE) with the argument type, which can take any of the four names of ST available in Vortex (i.e. Sampled, Latin Hypercube Sampling, Factorial or Single-Factor). This will influence the behaviour of the function as explained below. In this function too, it is possible to specify more than one point in time with the argument yrs. In Pairwise, it is also possible to set yrs="max" (default) in which case VortexR automatically set yrs to the last year of the simulation. Similarly as it was for the plot functions, params takes a character vector with the name of the parameters to be compared. For example, if the users set params="Nall" then the mean population size of each populations, for each scenario will be compared against the mean population size of the same population in the baseline scenario. 
The Pairwise function ranks, for each population, the scenarios (and SVs if relevant, see below) based on the absolute value of the statistics (either SC or SSMD) regardless of the sign. That is, the scenario with the absolute SC or SSMD value most different from zero will have a rank equal to '1'. The users will have to look to the actual statistics to evaluate the direction of the change. Lastly, the Kendall's coefficient of concordance is calculated to test whether the order of ranked scenarios (and SVs if relevant) is statistically consistent across the chosen points in time and parameters (and SVs). For example, if the user simulated 100 years and used Pairwise with yrs=c(50,100)and selected two parameters, say params=c("Nall", "Het"), the consistency of ranking will be tested across the four raters (i.e. Nall at year 50, and at year 100, Het at year 50 and at year 100). The Kendall's coefficient of concordance takes value between 0 and 1, where 0 is no agreement and 1 is complete agreement. A statistical test is performed and the p-values reported in the output. It should be noted that the Kendall's test operates a listwise deletion of missing data. However, when data in a whole column (i.e. ranks for a parameter) are missing, the column is removed before the statistic is calculated (see below notes on calculations of SC and SSMD for probability values).
When ST=FALSE, VortexR uses the argument scenario to identify the scenario that should be used as baseline scenario. When ST=TRUE, then VortexR automatically uses the scenario generated by Vortex with the Base values (whose name is ending with '(Base)') as baseline scenario. There may be situations where the users used a range of values for a few parameters in their simulations and want to evaluate the mean effect of each of these parameters on their outcome variables of interest (i.e. ranking the parameters, rather than scenarios). For example, let's say one has set up the simulations with five different carrying capacity (K) values and three different mortality rates for the adults of the organism being modelled, and is interested in evaluating the effects of these two parameters on the genetic diversity after x number of years. While comparing the single scenarios with the different values of K and adult mortality against the baseline scenario may be informative, Pairwise offers also the possibility to evaluate the overall 'mean' effect of varying K and adult mortality on genetic diversity. This is automatically done when the analysis is conducted on data generated when ST=TRUE, type ="Single-Factor" with more then one SV passed with the argument SVs. Otherwise, it is achievable in Pairwise with a combined use of the argument group.mean and SVs. The first argument tells VortexR to calculate la mean for each group, and SVs provides the names of the parameters that have to be used to group scenarios (by passing a character vector). For example, let's assume that the user has a ST run, with three SVs (SV1, SV2, and SV3) and 'Nall' and 'Het' are the outcome variables of interest. In this case by setting group.mean=T and passing SVs=c("SV1", "SV2", "SV3"), VortexR will first compare all the scenarios against the baseline scenario and then, following Conroy and Brook (2003), calculate the mean SC and SSMD for each group of scenarios that have different values from the base scenario of SV1, SV2, and SV3 (one at the time) and provide the ranks accordingly. The same would be achieved using ST=TRUE, type ="Single-Factor", SVs=c("SV1", "SV2", "SV3"). The values for the parameters passed with are evaluated by SVs VortexR at year=0. This is done because these parameters may take value 'zero' if some populations go extinct, which may cause problem when VortexR is trying to work out which parameters are different from the baseline. However, there are cases where Vortex may not evaluate these parameters even at year 0. This may happen, for example, when a population is empty at initialization (i.e. the initial population size is zero), or when K is set to zero at the beginning of the simulation. The user has to make sure that the values for the parameters passed in are correct, so that VortexR can group together scenarios appropriately.
Note that it only makes sense to rank the parameter in a ST run when the Single-Factor option is used in Vortex. This is because with Single-Factor, the parameters are modified one at the time. If, for example, the ST run used 'Factorial', all parameter combinations are used and therefore the ranking of parameter will also take into account the interaction of parameters and not only the single effects. In these cases, some manual manipulation of the data will be needed to remove the scenarios that are not needed. If the simulation parameters are not varied using a ST module, the users need to either set up adequate state variables in Vortex, so that relevant values of the simulation parameters are recorded, or add them manually to the output of the function CollateDat. Let's suppose that, in the example above where K and Adult mortality were varied, the user set up the simulations as 'normal' vortex scenarios (i.e. not with a ST module), but had created two population state variables, PS1 and PS2, where K and the adult mortality were respectively recorded. Because population state variables are automatically included in Vortex .dat or .stdat output, the user can easily obtain the mean SC and SSMD by passing group.mean=T, SVs=c("PS1", "PS2").
The sensitivity coefficients are calculated as (Drechsler et al. 1998):

where Vi is the mean value of the variable of interest in the scenario being compared, and VB is the mean value in the baseline scenario.
If the variable of interest is a probability (e.g. probability of extinction), then the logit is used (Drechsler et al. 1998) as follow:


The strictly standardised mean difference is calculated as (Zhang 2007):


with s being the standard deviation. In case variable of interest is a probability (e.g. probability of extinction), then the formula becomes:

Note that, in both statistics, the direction of the changes is inverted if SC or SSMD are calculated for probabilities. This is done to provide a more intuitive result, under the assumption that more often than not the users will be interested in the probability of extinction. Therefore a negative result reflects an increase in the probability of extinction of the scenario being compared to the baseline scenario.

The reader may have noter that when SC or SSMD are calculated for probabilities and one of these is zero for either the baseline scenario or the scenario being compared, the SC has little meaning (it will be either infinite or -infinite). Similarly, SC and SSMD can't be computed if both are zero or both their SD are zero, respectively.   
Sensitivity Coefficients have been around for a while and we included them in Pairwise in case the users are interested in comparing their results with previous work, but we also include the relatively new SSMD because it offers some advantages over SCs, most importantly allows for a statistical test and takes into account the variability of the results but it is not influenced by sample size. These points are better explained with an example. Let's imagine the user has a baseline scenario with mean population size NB=5,000, with a standard deviation SDB=500 and wants to compare two simulations where N1 = N2 = 4,000, but the standard deviations were SD1=100 and SD2=450.  Using the SC the two coefficients would be the same:
SC1 = SC2 = (4,000 - 5,000)/5,000 = -0.2
While using SSMD we would obtain two different values reflecting the fact that the second scenario has a larger fluctuation of N:
SSMD1 = (4,000 - 5,000)/√(1002+5002) = 1.961
SSMD2 = (4,000 - 5,000)/√(4502+5002) = 1.487
Moreover,  we can calculate the p-value associated with the two SSMD values and verify that with SSMD1 we can be confident that there is a statistical difference between the N of this scenario and the baseline scenario (p-value = 0.025) while there is no statistical difference at all (p-value = 0.069) with the second scenario. Note that if we were to calculate the p-values of the t-tests between the N1 and N2 with NB, assuming that the user generated the results using (the quite standard) 1,000 iterations, both t-tests would result in p<0.0001, providing very little insight into the potentially biological important differences between these scenarios with the baseline scenario.  This is so because the statistical significance of the t-test is influenced not only by the mean difference, but also by the sample size, which is typically large because ≥1000 iterations are usually performed.
Pairwise returns a list where each result is an element. The first six elements are always present, and these are: a dataframe with SC values for all scenarios, a dataframe with SSMD values, a dataframe with p-values for SSMD values, a dataframe with the scenario ranks based on SC and one based on SSMD and the output of the Kendall's test. If group.mean=T there will be six additional elements: a dataframe with the mean SC values for each parameter, a dataframe with the mean SSMD values, a dataframe with p-values calculated for the mean SSMD values, a dataframe with the parameter ranks based on the mean SC and one based on the mean SSMD and the output of the Kendall's test performed on the ranking of the parameters.
All these results are also saved to disk, in separated files (each in two formats, .RData and .txt) in a dedicated folder "DataAnalysis".

A worked example:
# Read data from all .stdat of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_Classic' and 
# store the output in the object 'col.st.classic', 
# which is needed to run the function Pairwise
col.st.classic <- CollateDat(project="Pacioni_et_al", scenario="ST_Classic",
                             ST=TRUE, runs=3, filepath=getwd())

pairw<-Pairwise(data=col.st.classic, project="Pacioni_et_al",
                scenario="ST_Classic", params=c("Nall", "Het"), yrs="max",
                ST=T, type="Single-Factor",
                SVs=c("SV1", "SV2", "SV3", "SV4", "SV5", "SV6", "SV7"))

Regression
The Regression function fits a regression model to the data that Vortex generated. It is very difficult to predict what kinds of analyses Vortex users will carry out. With Regression we provide a first attempt to automise a general work-flow that we believe will be quite commonly used, but it should be considered still experimental and under development. At this stage, the function can take two inputs: demographic data from census files (.yr) and run files (.run), both reorganised in long format. Please, note that Regression has not been tested to analyse data from .dat and .stdat, this option may become available in future releases. The users indicate the type of input with the argument census (TRUE – the default – for census data). Remember that parameters used to set up ST runs in Vortex are called GSx in the census files (but these are called SVx in the data file, .stdat), this will be relevant later. 
In case data from census files are used, the users have also to indicate, with the argument yr, the year of the simulations that needs to be used. For example, if a user wanted to conduct an analysis for two points in time, year X and Y, he or she would have re-formatted the census data in a long format with the function ConvLYr using these values for the argument yrs (i.e. yrs=c(X, Y)) .  The default value for yr in Regression is NA , to avoid that the users inadvertently omits this parameter, so the users have to pass an integer even when only one year of the simulations had been retained in the input data. 
With the argument popn, the users pass the population(s) that needs to be analysed. The population(s) is identified with a numeric value that reflects the position in which it is listed in Vortex's project. For example, in a two population project, popn=1 would indicate the first population, popn=2, the second and popn=3 the Metapopulation. 
Regression fits a different type of regression model depending on the dependent variable, which is passed with the argument param. When this is a count (e.g. N or the number of alleles) the function will fit a Generalized Linear Model. The first fit is attempted with a Poisson error distribution and if c^ (dispersion parameter, calculated as  ) is larger than (the somewhat arbitrary cut off of) 1.5, the model will be refitted with a quasipoisson error distribution (a message is displayed if this happens). If the users generated their own dependent variable (e.g. through a PS), this has to be listed in the count.data argument to indicate Regression that it is analysing count data. Default values for count.data are all the count-data variables that are automatically generated in Vortex outputs:
count.data=c("Nextant", "Nall", "Nalleles", "N", "AM", "AF", "Subadults",
		"Juv", "nDams", "nBroods", "nProgeny", "nImmigrants",
		"nEmigrants", "nHarvested", "nSupplemented", "YrExt", 
		"Alleles")

Predictors are passed as character vector with the argument vs. In the initial fit of the model the main and interactions effects are included. Successively, a search for the best model is carried out. This is performed with the package glmulti (Calcagno & de Mazancourt 2010). A brief description of this package is provided below, but the users are recommended to consult glmulti documentations to have a clear understanding of the functionality of this package. The search will be exhaustive if the number of possible candidate models is less than the cut off value passed with the n.cand argument (default: 30). Otherwise the search is conducted using the genetic method, which is an efficient, but non-exhaustive, method (see Calcagno & de Mazancourt 2010 for details). In this step, the search includes only the main effects if l=1 or all pairwise effects if l=2. The models are ranked based on the information criterion passed with ic (default AIC). The function saves a file with the name ending with "best.mod.RData" and returns it as glmulti object called "best.mod". Note that R will generate a warning if the param is a count but there are values that are not integers. This is important if you trying to pass to this function data collected using other functions or in a different format than Regression is expecting. For example, while it is theoretically possible to pass to Regression the collated data from either .dat or .stdat files (with the function CollateDat), R will complain if the dependent variable is N as in this case it is a mean of the population sizes across iterations and will have decimal digits (a similar problem happens also when the dependent variable is "Alleles", see below). It is up to the user to make sure that the format of the data is adequate and that the headings of the columns are those that VortexR is expecting. 
The argument set.size is used to set the number of models that the search should return. The default is NA and if no alternative value is passed, VortexR will use whichever is the smaller between the number of possible candidate models and n.cand.
If param is a variable that takes proportions (i.e. Gene Diversity and Inbreeding), then the function uses a Beta regression from the R package betareg (Cribari-Neto & Zeileis 2010). Initially VortexR tests different link functions and selects the one with the lowest AIC value. The selected link function is displayed on the R console and the difference in the AIC scores relative to the best link function is also displayed. Then glmulti carries out the search for the best models as described above. Regression explicitly ignores NA (for example when the population goes extinct and the user requested Gene Diversity as dependent variable). Depending on the data, fitting several Beta regression models to complete the search may be a long (and memory hungry) process. Also, the user should keep in mind that the package betareg has the limitation that currently cannot handle analysis of data when the dependent variable takes value of either exactly 0 or 1.
When using data output in the .run files, because the .run files do not store values from additional variables (e.g. SVx or user-made PS), the user has also to provide a "look-up table". A look-up table is a table where the scenario names are listed together with the (missing) variables needed to fit the regression models. VortexR will match the scenarios contained in the input data with the relevant information contained in the look-up table. A look-up table can be generated with the LookUpTable function and be passed with the argument lookup (but remember that parameters used to set up ST runs in Vortex are called SVx in the data file, .stdat). Alternatively, this information can be manually added by the users to the input data. In this case, lookup can be left on its default value NA. 
Another situation when the users may want to pass the variables with the argument lookUp even when they are using the census data is when some of the populations go extinct. In this situation the values for the dependent variables may take value 'zero'. By using the output from LookUpTable, the parameters are evaluated at year 0 and should then take the appropriate values.
Also, when the number of alleles is the dependent variable (from .run files), this is rounded to integer (to meet R requirement that count data are integers) before a GLM is fitted to the data because it is reported as a mean in the .run files and will have decimal digits.
While Regression is running, a summary of the dependent variable is displayed, which can be useful to check possible problems. 
The output of the function is saved in a "Regression" folder (which is created if it is not there already). In addition to the already mentioned glmulti object, with the results of the model search for the best models, a histogram is saved (as PDF) to check the distribution of the dependent variable. When the search for the best models is completed, a plot of the information criterion values is also saved (again as PDF). The names of all these files will start with the combination of the Project and Scenario names, followed by the suffix "histogram.pdf", "IC_plot.pdf" or "best.mod.RData". By loading the latter object back in R, the user can extract a number of information and data, which are detailed in glmulti documentations (a few examples are provided in the A worked example section). A very nice option that glmulti offers is the possibility to easily obtain model average estimates. This functionality is currently (most likely) compromised for most users using betareg (but available for GLMs). This is because the default types of residuals (sweighted2) in betareg are computationally demanding and R may fail because not sufficient memory is available. In theory this is easily avoided using alternative methods to calculate residuals (e.g. type="pearson"), but it is not currently possible to pass additional arguments in glmulti when coef.glmulti() is used. This is something that is being evaluated by the glmulti package developer and may change in the future.  

A worked example
# Read data from all .stdat of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_LHS' and 
# store the output in the object 'col.ST.LHS', 
col.ST.LHS <- CollateDat(project="Pacioni_et_al", scenario="ST_LHS",
                           ST=TRUE, runs=3, filepath=getwd())

# Remove base scenario
col.ST_LHS.no.base <- col.ST.LHS[!col.ST.LHS$scen.name == "ST_LHS(Base)",]

# Use function LookUpTable to obtain correct parameter values at year 0,
# which is needed to run the function Regression

lkup.ST_LHS <- LookUpTable(data=col.ST_LHS.no.base, project="Pacioni_et_al",
                        scenario="ST_LHS", ST=TRUE,
                        pop="Population 1",
                        SVs=c("SV1", "SV2", "SV3", "SV4", "SV5", "SV6", 				"SV7"))

# Run CollateRun on all .run of the project 'Pacioni_et_al' and
# the ST scenario 'ST_LHS' in the selected folder.
# Store the result in 'run', which is used as input for Regression
run <- CollateRun(project="Pacioni_et_al", scenario="ST_LHS", numPops=1)

# Remove base scenario from the putput in long format
lrun.ST_LHS.no.base <- run[[2]][!run[[2]]$Scenario == "ST_LHS(Base)",]

# Regression
reg <- Regression(data=lrun.ST_LHS.no.base, lookUp=lkup.ST_LHS, census=F,
                  project="Pacioni_et_al", scenario="ST_LHS", popn=1,
                  param="N",
                  vs=c("SV1", "SV2", "SV3", "SV7"),
                  l=2,  n.cand=30)

# Example of information you can obtained once you have run Regression
# The formula for the best model
bestmodel <- reg@formulas[1]
# The formulae for the best 30 model
bestmodels <- reg@formulas
# List of QAIC values
qaicvalues<-reg@crits
# QAIC differences between the first 5 best models
delta <- as.vector(NULL)
for (i in 1:5) {
  del <- qaicvalues[i+1] - qaicvalues[i]
  delta <- c(delta, del)
}

# The best model's coefficients
coef.best <- coef(reg@objects[[1]])
# The model avereaged coefficients
coef.all <- coef.glmulti(reg)
coefs <- data.frame(Estimate=coef.all[,1], Lower=coef.all[,1] - coef.all[,5], Upper=coef.all[,1] + coef.all[,5])

# Capture outputs from above in text file named Regression_Results.txt
capture.output(print("Best model"),  print(bestmodel),
               print("Top five models")  , print(bestmodels[1:5]),
               print("Delta IC for the first 6 models"), print(delta),
               print("Coefficients for the best model"), print(coef.best),
               print("Model averaged coefficients"), print(coefs),
               file="./Regression/Regression_Results.txt")

# Plot of model averaged importance of terms
plot(reg, type="s")
pdf("./Regression/Reg_importance_plot.pdf")
plot(reg, type="s")
dev.off()

Utilities
CollateProcData
The function CollateProcData (Collate Processed Data) is used to collate processed data by any of the Collate functions. This may be useful when, for example, the user wants to have all the data generated by different ST scenarios and 'standard' scenario runs in standard modality into a unique dataframe that can then be passed to other functions. The user has to prepare the data by generating a list where each dataframe is an element and pass the name of the list with the argument data. The output will be a dataframe that contains all the data. Missing data will be filled with NA. The function also saves the results as .RData and .txt with the name "CombinedDB" in the folder "ProcessData".

A worked example:
# First, the data needs to be read in and then merged in one dataframe.

#Read data from all .dat of the project "Starlingv3PopBased" 
# and store the output in the object "col"
col<-CollateDat(project="Starlingv3PopBased", ST=F, runs=10000)

# Read data from all .stdat of the project "Starlingv3PopBased" 
# and ST scenario "MReductEvy5" and store the output in the object "colst"
colst<-CollateDat(project="Starlingv3PopBased", scenario="MReductEvy5", ST=F, runs=10000)

# Now we can merge the data together. Generate a list with the data
dfs[[1]] <- col
dfs[[2]] <- colst
# Then merge with CollProcData
db <- CollateProcData(data=dfs)

LookUpTable
This is a small function that simply takes the output from CollateDat (with the argument data) and trims it down to one entry per scenario. The user has to indicate the name of the population to use as reference with the argument pop and indicate the parameters to be retained in the final table using SVs. The scope of this function is to create a table that summarises simulation parameters. It was initially thought for ST runs where the user may want to have a table that reports the values of the SVs for each scenario (hence the name of the argument SVs!), but because SVs takes a character vector, the user can request any parameter included in the output of CollateDat. This function reports the values of SVs at year zero. This is done because these parameters may take value 'zero' if some populations go extinct. However, there are cases where Vortex may not evaluate these parameters even at year 0. This may happen, for example, when a population is empty at initialization (i.e. the initial population size is zero), or when K is set to zero at the beginning of the simulation. The user should check the values reported and check the Vortex input files if these do not look correct. 

A worked example:
# Read data from all .stdat of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_Classic' and 
# store the output in the object 'col.st.classic', 
# which is needed to run the function LookUpTable
col.st.classic <- CollateDat(project="Pacioni_et_al", scenario="ST_Classic",
                             ST=TRUE, runs=3, filepath=getwd())

lkup.st.classic <- LookUpTable(data=col.st.classic, project="Pacioni_et_al",
                    scenario="ST_Classic", ST=TRUE, pop="Population 1",
                    SVs=c("SV1", "SV2", "SV3", "SV4", "SV5", "SV6", "SV7",
                          "PS1", "PS2", "PS3", "PS4"))


Nb
The 'CensusMeans' output obtained from the function CollateYr can be passed as input to Nb. The total number of populations contained in the input file is passed with numPopsNoMeta. Remember that the 'CensusMeans' does not contain data for the metapopulation, even if there are more than one population in the simulations. However, data for the metapopulation can be generated by setting to TRUE the argument appendMeta (as it was for ComvLYr).
This function is used to calculate the harmonic mean of the total number of adults between two points in time, which are indicated with the argument yr0 and yrt: the first and the last year, respectively, that should be used by the function for the calculations. Because this function was written to primarily allow the calculation of the Ne:N ratio (where Ne is the effective population size), the user has to be careful that the census size (number of breeders, technically) calculated with this function is relevant for the Ne being used.  Following Wapple's (2005) recommendations, the function adjusts the yrt value by subtracting the number of years of the generation time that the user has passed with the argument gen (rounded to the nearest integer. Note that vortex calculates the generation time of the organism being modelled as part of the deterministic calculations). This is because the Ne calculated with the temporal approach will be influenced by the breeding pool of the previous generation, therefore this adjustment makes sure that the temporal windows of the two functions (Nb and Ne) when using the same parameters is the same and direct calculation of the ratio is meaningful (see 'A worked example' section for the function Ne to see how this can be done). We implemented the function in this way because we believe that, more often than not, this would suit the user' needs, but ultimately it is the user responsibility to evaluate an appropriate approach for the specific organism being modelled. In case the user would like the function not to adjust the yrt value (e.g. for organisms with discrete generations), it is possible to set gen=0 to either (or both) the functions. 
By default, all scenarios included in the input data are considered. However there is the possibility to limit  the scenarios considered by passing a character vector that lists the scenarios that the user wants to be included with the argument scenarios (e.g. scenarios=c("scen1", "scen2")). 
A worked example:
# Run CollateYr on all .yr of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_Classic' in the selected folder and 
# store the result in ' yr.st.classic '
yr.st.classic <- CollateYr(project="Pacioni_et_al", scenario="ST_Classic",
                           numPopsNoMeta=1)

# Calculate effective pop size using demographic data
NbAll <- Nb(data=yr.st.classic[[2]], scenarios="all", numPopsNoMeta=1, 			gen=2.54, yr0=50, yrt=120)

Ne
Ne is a function that calculates Ne (effective population size) based on the loss of genetic diversity (expected heterozygosity) using the temporal approach. It takes the dataframe generated with CollateDat (which is passed with the argument data). The user can indicate the scenario(s) for which the calculations should be carried out by passing a character vector to the argument scenarios otherwise, by default, all scenarios are considered. With the arguments yr0, yrt and gen the user indicates the time window to be considered (first and last year respectively) and the generation time (note that vortex calculates the generation time of the organism being modelled as part of the deterministic calculations). In Ne, yr0 is adjusted by adding the number of years of the generation time (rounded to the nearest integer).  In this way the user can provide the same yr0, yrt and gen to Nb and Ne and these values are adjusted internally so that it is possible to directly divide the results to correctly calculate the ratio. The user should be careful when using this function because if a population goes extinct, the result of the calculations is 0.5. On the other hand, if the population size of a population has at is zero, the result of the calculations is 0. These behaviours may change in future version of VortexR, but in the meantime, the users should make sure they do not get spurious results because of an unsuitable input. 
A worked example:
# Read data from all .stdat of the project 'Pacioni_et_al' and 
# the ST scenario 'ST_Classic' and 
# store the output in the object 'col.st.classic', 
# which is needed to run the function Ne
col.st.classic <- CollateDat(project="Pacioni_et_al", scenario="ST_Classic",
                             ST=TRUE, runs=3, filepath=getwd())

# Calculate effective pop size using genetic data
NeAll <- Ne(data=col.st.classic, scenarios="all", gen=2.54, yr0=50, yrt=120)

# With only one population in your simulations, the following
# will calculate Ne:N for all scenarios.


# load required package
require(data.table)

# set the key in NbAll (from Nb example) to extract the pop you want 
# to calculate Ne for
setkey(NbAll, Population)

# Calculate the ratios
NeNRatio <- data.table(NeAll[ , "Population 1", with=F]/NbAll["pop1", Nb],
                       Scenario=NeAll[ , Scenario])

# Write the results in a csv file
write.csv(NeNRatio, file="./DataAnalysis/NeNRatio.csv", row.names=F)






# If you have more populations and you want to calculate Ne:N for all of
# them at once, the following should do...

# load required packages
require(reshape2)
require(data.table)

# This stack data based on Scenario' names
NeStacked <- melt(NeAll, id="Scenario")

# set the key and name coloumns
setkey(NeStacked, Scenario)
setnames(NeStacked, c("variable", "value"), c("Population", "Ne"))

# Calculate the ratios
NeNRatio <- data.table(Scenario = NeStacked[ , Scenario], 
            Population = NeStacked[ , Population], 
            NeNRatio = NeStacked[, Ne]/NbAll[, Nb])

Acknowledgements
We are deeply indebted to Dr Robert Lacy, who has continuously helped to clarify functions and details about Vortex during the development of VortexR. Moreover, he has been always opened to exchange opinions and ideas, generating many interesting conversations.
In a way, lots of people have contributed to VortexR through online forums (mainly stackoverflow.com). Suggestions and ideas provided in this huge source of knowledge have been used to find a way, or a better way, to achieve what we were trying to do. We feel that a global "thank you" is in order...
Some of the codes included in VortexR were initially developed by CP as part of independent projects funded by Whiteman Park, the Western Australia Department of Agriculture and Food, and the Western Australia Department of Parks and Wildlife. We are thankful to these agencies for agreeing with the re-adaptation of these codes for their use in VortexR. 

References
Calcagno, V., and C. de Mazancourt. 2010. glmulti: an R package for easy automated model selection with (generalized) linear models. Journal of Statistical Software 34:1-29.
Conroy, S. D. S., and B. W. Brook. 2003. Demographic sensitivity and persistence of the threatened white- and orange-bellied frogs of Western Australia. Population Ecology 45:105-114.
Cribari-Neto, F., and A. Zeileis. 2010. Beta regression in R.
Drechsler, M., M. A. Burgman, and P. W. Menkhorst. 1998. Uncertainty in population dynamics and its consequences for the management of the orange-bellied parrot Neophema chrysogaster. Biological Conservation 84:269-281.
Lacy, R. C., and J. P. Pollak. 2013. VORTEX: a stochastic simulation of the extinction process. Version 10. Chicago Zoological Society, Brookfield.
M Dowle, T Short, S Lianoglou, and A. S. w. c. f. R. S. a. E. Antonyan. 2014. data.table: Extension of data.frame. R package version 1.9.2. http://CRAN.R-project.org/package=data.table.
Waples, R. S. 2005. Genetic estimates of contemporary effective population size: to what time periods do the estimates apply? Molecular Ecology 14:3335-3352.
Zhang, X. D. 2007. A pair of new statistical parameters for quality control in RNA interference high-throughput screening assays. Genomics 89:552-561.



Appendix: VortexR functions at glance



dropdown.R  guess the names of the project and the names on the scenarios. It works correctly only if the users don't use "_" (underscores) in the name of the project.


## Troubleshooting:
Error in file cannot open the connection: The most likely cause of this error message is that you have run the analysis already before and have opened some of the output files (e.g. in excel) and are repeating the analysis. In such a case, R cannot access the file because another program is using it.

# Example workflow
```{r}
#  Set working directory
# setwd("C:/Users/30373314/Documents/VortexR/Examples/VOutput")
# setwd(choose.dir())

# Load (Source for now) VortexR
# source("C:/Users/30373314/Documents/VortexR/vortexr/R/VortexR.R")
# source("VortexR.R")
library(vortexr)

# # Read data from 'Pacioni_et_al_ST_Classic(Base).stdat' and store the output
# # in the object 'col.classic.base', which is available in memory.
# # The function will also save two output files (see above).
# col.st.classic <- CollateOneDat("pacioni/Pacioni_et_al_ST_Classic(Base).stdat", 3)
# 
# 
# # Read data from all .stdat of the project 'Pacioni_et_al' and the ST scenario
# # 'ST_Classic' and store the output in the object 'col.st.classic'
# col.st.classic <- CollateDat(project="Pacioni_et_al", scenario="ST_Classic",
#                              ST=TRUE, runs=3, filepath=getwd())
# 
# # Plot data
# dot <- DotPlot(data=col.st.classic, project="Pacioni_et_al", scenario="ST_Classic",
#                ST=TRUE, yrs=c(80, 120),
#                params=c("PExtinct", "Nextant", "Het", "Nalleles"),
#                setcolour="scen.name")
# 
# lineplot.st.classic <- LinePlotYear(data=col.st.classic, project="Pacioni_et_al",
#                          scenario="ST_Classic", ST=TRUE,
#                          params=c("PExtinct", "Nextant", "Het", "Nalleles"))
# 
# lineMidPlot.st.classic <- LinePlotYearMid(data=col.st.classic, project="Pacioni_et_al",
#                               scenario="ST_Classic", ST=TRUE, yrmid=50,
#                               params=c("PExtinct", "Nextant", "Het", "Nalleles"))
# 
# lkup.st.classic <- LookUpTable(data=col.st.classic, project="Pacioni_et_al",
#                     scenario="ST_Classic", ST=TRUE, pop="Population 1",
#                     SVs=c("SV1", "SV2", "SV3", "SV4", "SV5", "SV6", "SV7",
#                           "PS1", "PS2", "PS3", "PS4"))
# 
# pairw<-Pairwise(data=col.st.classic, project="Pacioni_et_al",
#                 scenario="ST_Classic", params=c("Nall", "Het"), yrs="max",
#                 ST=T, type="Single-Factor",
#                 SVs=c("SV1", "SV2", "SV3", "SV4", "SV5", "SV6", "SV7"))
# 
# # Run CollateYr on all .yr of the project 'Pacioni_et_al' and the ST scenario
# # 'ST_Classic' in the selected folder and store the result in 'yr.st.classic'
# yr.st.classic <- CollateYr(project="Pacioni_et_al", scenario="ST_Classic",
#                            numPopsNoMeta=1)
# 
# # Calculate effective pop size using demographic data
# NbAll <- Nb(data=yr.st.classic[[2]], scenarios="all", numPopsNoMeta=1, gen=2.54,
#   yr0=50, yrt=120)
# 
# # Calculate effective pop size using genetic data
# NeAll <- Ne(data=col.st.classic, scenarios="all", gen=2.54, yr0=50, yrt=120)
# 
# # With only one population in your simulations, the following
# # will calculate Ne:N for all scenarios.
# 
# 
# # load required package
# require(data.table)
# 
# # set the key in NbAll to extract the pop you want to calculate Ne for
# setkey(NbAll, Population)
# 
# # Calculate the ratios
# NeNRatio <- data.table(NeAll[ , "Population 1", with=F]/NbAll["pop1", Nb],
#                        Scenario=NeAll[ , Scenario])
# 
# # Write the results in a csv file
# write.csv(NeNRatio, file="./DataAnalysis/NeNRatio.csv", row.names=F)
# 
# 
# 
# ########## - ST LHS - ##########
# 
# col.ST.LHS <- CollateDat(project="Pacioni_et_al", scenario="ST_LHS",
#                            ST=TRUE, runs=3, filepath=getwd())
# 
# # Remove base scenario
# col.ST_LHS.no.base <- col.ST.LHS[!col.ST.LHS$scen.name == "ST_LHS(Base)",]
# 
# # Plots
# lineplot.ST_LHS <- LinePlotYear(data=col.ST_LHS.no.base, project="Pacioni_et_al",
#                             scenario="ST_LHS", ST=TRUE,
#                             params=c("PExtinct", "Nextant", "Het", "Nalleles"))
# 
# # Use function LookUpTable to obtain correct parameter values at year 0
# lkup.ST_LHS <- LookUpTable(data=col.ST_LHS.no.base, project="Pacioni_et_al",
#                         scenario="ST_LHS", ST=TRUE,
#                         pop="Population 1",
#                         SVs=c("SV1", "SV2", "SV3", "SV4", "SV5", "SV6", "SV7"))
# 
# scatter.plot <- MScatter(data=col.ST_LHS.no.base[1:33], data.type="dat",
#                          lookUp=lkup.ST_LHS, yr=120, popn=1, param="Nall",
#                          vs=c("SV1", "SV2", "SV3", "SV4", "SV5", "SV6", "SV7"))
# 
# # Run CollateRun on all .run of the project 'Pacioni_et_al' and
# # the ST scenario 'ST_LHS' in the selected folder.
# # Store the result in 'run'
# run <- CollateRun(project="Pacioni_et_al", scenario="ST_LHS", numPops=1)
# # Remove base scenario from the putput in long format
# lrun.ST_LHS.no.base <- run[[2]][!run[[2]]$Scenario == "ST_LHS(Base)",]
# 
# # Regression
# reg <- Regression(data=lrun.ST_LHS.no.base, lookUp=lkup.ST_LHS, census=F,
#                   project="Pacioni_et_al", scenario="ST_LHS", popn=1,
#                   param="N",
#                   vs=c("SV1", "SV2", "SV3", "SV7"),
#                   l=2,  n.cand=30)
# 
# # Example of information you can obtained once you have run Regression
# # The formula for the best model
# bestmodel <- reg@formulas[1]
# # The formulae for the best 30 model
# bestmodels <- reg@formulas
# # List of QAIC values
# qaicvalues<-reg@crits
# # QAIC differences between the first 5 best models
# delta <- as.vector(NULL)
# for (i in 1:5) {
#   del <- qaicvalues[i+1] - qaicvalues[i]
#   delta <- c(delta, del)
# }
# 
# # The best model's coefficients
# coef.best <- coef(reg@objects[[1]])
# # The model avereaged coefficients
# coef.all <- coef.glmulti(reg)
# coefs <- data.frame(Estimate=coef.all[,1], Lower=coef.all[,1] - coef.all[,5], Upper=coef.all[,1] + coef.all[,5])
# 
# # Capture output in text file named Regression_Results.txt
# capture.output(print("Best model"),  print(bestmodel),
#                print("Top five models")  , print(bestmodels[1:5]),
#                print("Delta IC for the first 6 models"), print(delta),
#                print("Coefficients for the best model"), print(coef.best),
#                print("Model averaged coefficients"), print(coefs),
#                file="./Regression/Regression_Results.txt")
# 
# # Plot of model averaged importance of terms
# plot(reg, type="s")
# pdf("./Regression/Reg_importance_plot.pdf")
# plot(reg, type="s")
# dev.off()

```




